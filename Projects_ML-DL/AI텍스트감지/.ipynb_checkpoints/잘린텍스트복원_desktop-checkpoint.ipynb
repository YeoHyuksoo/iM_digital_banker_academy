{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82102\\PycharmProjects\\iM_ML-DL\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40432 entries, 0 to 40431\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   category  40432 non-null  object \n",
      " 1   rating    40432 non-null  float64\n",
      " 2   label     40432 non-null  object \n",
      " 3   text_     40432 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. 데이터 확인 및 전처리\n",
    "# Null 값 확인 및 제거\n",
    "print(data.info())\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 텍스트와 라벨 분리\n",
    "texts = data['text_']\n",
    "labels = data['label']\n",
    "\n",
    "# 'CG'를 1로, 'OR'을 0으로 변환\n",
    "labels = labels.map({'CG': 1, 'OR': 0}).astype(np.float32)\n",
    "\n",
    "# 3. 텍스트 데이터 전처리\n",
    "# 토크나이저 정의\n",
    "max_words = 10000  # 사용할 최대 단어 수\n",
    "max_len = 100  # 리뷰의 최대 길이\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 텍스트 시퀀스 변환 및 패딩\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 4. 데이터셋 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. 모델 생성\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7. 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 8. 모델 저장\n",
    "model.save(\"ai_human_review_classifier.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잘린 텍스트 생성하기 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (369639, 296), y shape: (369639,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 'CG' 레이블 필터링\n",
    "cg_data = data[data['label'] == 'CG']\n",
    "\n",
    "# 마침표로 끝나지 않는 텍스트 필터링\n",
    "cg_data_complete = cg_data[cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "# 마침표로 끝나는 텍스트만 학습 데이터로 사용\n",
    "cg_data_incomplete = cg_data[~cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(texts):\n",
    "    return [text.lower() for text in texts]\n",
    "\n",
    "# 학습용 데이터 준비\n",
    "texts = preprocess_text(cg_data_complete['text_'].tolist())\n",
    "# texts = cg_data_complete['text_'].tolist()\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 시퀀스 생성\n",
    "input_sequences = []\n",
    "for sentence in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 패딩 처리\n",
    "max_seq_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# 입력(X)과 출력(Y) 분리\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 296, 50)           369700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 296, 64)           29440     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7394)              480610    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 912,774\n",
      "Trainable params: 912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length-1),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    LSTM(units=64),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1444/1444 [==============================] - 1596s 1s/step - loss: 1.8473 - accuracy: 0.5562\n",
      "Epoch 2/15\n",
      "1444/1444 [==============================] - 1618s 1s/step - loss: 1.8395 - accuracy: 0.5574\n",
      "Epoch 3/15\n",
      "1444/1444 [==============================] - 1569s 1s/step - loss: 1.8314 - accuracy: 0.5588\n",
      "Epoch 4/15\n",
      "1444/1444 [==============================] - 1576s 1s/step - loss: 1.8244 - accuracy: 0.5602\n",
      "Epoch 5/15\n",
      "1444/1444 [==============================] - 1595s 1s/step - loss: 1.8169 - accuracy: 0.5618\n",
      "Epoch 6/15\n",
      "1444/1444 [==============================] - 1617s 1s/step - loss: 1.8108 - accuracy: 0.5630\n",
      "Epoch 7/15\n",
      "1444/1444 [==============================] - 1630s 1s/step - loss: 1.8038 - accuracy: 0.5647\n",
      "Epoch 8/15\n",
      "1444/1444 [==============================] - 1646s 1s/step - loss: 1.7970 - accuracy: 0.5661\n",
      "Epoch 9/15\n",
      "1444/1444 [==============================] - 1669s 1s/step - loss: 1.7908 - accuracy: 0.5674\n",
      "Epoch 10/15\n",
      "1444/1444 [==============================] - 1677s 1s/step - loss: 1.7834 - accuracy: 0.5691\n",
      "Epoch 11/15\n",
      "1444/1444 [==============================] - 1690s 1s/step - loss: 1.7775 - accuracy: 0.5701\n",
      "Epoch 12/15\n",
      "1444/1444 [==============================] - 1701s 1s/step - loss: 1.7716 - accuracy: 0.5714\n",
      "Epoch 13/15\n",
      "1444/1444 [==============================] - 1715s 1s/step - loss: 1.7651 - accuracy: 0.5729\n",
      "Epoch 14/15\n",
      "1444/1444 [==============================] - 1715s 1s/step - loss: 1.7599 - accuracy: 0.5739\n",
      "Epoch 15/15\n",
      "1444/1444 [==============================] - 1705s 1s/step - loss: 1.7534 - accuracy: 0.5749\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 256\n",
    "history = model.fit(X, y, epochs=epochs, batch_size = batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Makes may tea with out stirring. The only problem is that it's kind of hard to put\n",
      "Generated Text: Makes may tea with out stirring. The only problem is that it's kind of hard to put the bottom on top the plastic part is also very light and easy to wash i have used it twice\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, seed_text, max_length=20):\n",
    "    for _ in range(max_length):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        # 예측된 값에서 가장 확률이 높은 단어 인덱스 선택\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "        \n",
    "        # 예측된 인덱스를 단어로 변환\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        \n",
    "        # 단어를 시드 텍스트에 추가\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "        # 마침표가 나오면 텍스트 생성 종료\n",
    "        if output_word == '.':\n",
    "            break\n",
    "    \n",
    "    return seed_text\n",
    "\n",
    "# 마침표가 없는 리뷰 중 하나를 가져와 뒷부분 생성\n",
    "test_text = cg_data_incomplete.iloc[10]['text_']\n",
    "generated_text = generate_text(model, tokenizer, test_text)\n",
    "\n",
    "print(f\"Original Text: {test_text}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"recover_model_lstm_x2.5.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 296, 50)           369700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 296, 64)           29440     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7394)              480610    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 912,774\n",
      "Trainable params: 912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('recover_model_lstm_x2.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
