{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e685ddda-8e4d-4eaf-9409-1e14c3891bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Human Reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\PycharmProjects\\iM_ML-DL\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 592ms/step - loss: 2.7238 - mae: 1.2206 - val_loss: 1.3361 - val_mae: 0.9088\n",
      "Epoch 2/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 623ms/step - loss: 1.6098 - mae: 0.9891 - val_loss: 1.3544 - val_mae: 0.8787\n",
      "Epoch 3/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 525ms/step - loss: 1.6021 - mae: 0.9832 - val_loss: 1.3340 - val_mae: 0.8916\n",
      "Epoch 4/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 639ms/step - loss: 1.6776 - mae: 1.0053 - val_loss: 1.3300 - val_mae: 0.9155\n",
      "Epoch 5/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 560ms/step - loss: 1.5889 - mae: 0.9708 - val_loss: 1.3338 - val_mae: 0.9253\n",
      "Epoch 6/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 628ms/step - loss: 1.5714 - mae: 0.9679 - val_loss: 1.3918 - val_mae: 0.9490\n",
      "Epoch 7/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 672ms/step - loss: 1.4830 - mae: 0.9415 - val_loss: 1.3159 - val_mae: 0.8996\n",
      "Epoch 8/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 607ms/step - loss: 1.4860 - mae: 0.9401 - val_loss: 1.3257 - val_mae: 0.9133\n",
      "Epoch 9/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 692ms/step - loss: 1.4616 - mae: 0.9296 - val_loss: 1.3999 - val_mae: 0.8494\n",
      "Epoch 10/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 525ms/step - loss: 1.3771 - mae: 0.9020 - val_loss: 1.3424 - val_mae: 0.8837\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 325ms/step - loss: 1.4100 - mae: 0.9245\n",
      "Validation Loss: 1.3159157037734985, Validation MAE: 0.8995621800422668\n",
      "Training on AI Reviews...\n",
      "Epoch 1/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2904s\u001b[0m 6s/step - loss: 2.6800 - mae: 1.2139 - val_loss: 1.2956 - val_mae: 0.9085\n",
      "Epoch 2/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 795ms/step - loss: 1.6211 - mae: 0.9892 - val_loss: 1.2940 - val_mae: 0.9004\n",
      "Epoch 3/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 773ms/step - loss: 1.5765 - mae: 0.9723 - val_loss: 1.3288 - val_mae: 0.9394\n",
      "Epoch 4/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 713ms/step - loss: 1.5405 - mae: 0.9690 - val_loss: 1.3107 - val_mae: 0.8703\n",
      "Epoch 5/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 570ms/step - loss: 1.5786 - mae: 0.9732 - val_loss: 1.2987 - val_mae: 0.9082\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 298ms/step - loss: 1.2191 - mae: 0.8857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.293982744216919, Validation MAE: 0.9004335999488831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "data_path = 'fake_reviews_dataset.csv'  # 데이터 파일 경로\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 2. 인간과 AI 리뷰 분리\n",
    "human_reviews = data[data['label'] == 'OR']\n",
    "ai_reviews = data[data['label'] == 'CG']\n",
    "\n",
    "# 3. 데이터 전처리 함수 정의\n",
    "def preprocess_data(df, max_words=10000, max_len=200):\n",
    "    texts = df['text_'].astype(str).values\n",
    "    ratings = df['rating'].values\n",
    "\n",
    "    # 텍스트 토큰화\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "    return padded_sequences, ratings, tokenizer\n",
    "\n",
    "# 4. 모델 정의 함수\n",
    "def build_lstm_model(max_words=10000, max_len=200, embedding_dim=128):\n",
    "    model = Sequential([\n",
    "        Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "        LSTM(128, return_sequences=True, dropout=0.2),\n",
    "        LSTM(64, dropout=0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # 회귀 문제\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# 5. 학습 및 평가\n",
    "def train_and_evaluate(df, max_words=10000, max_len=200, embedding_dim=128, test_size=0.2, batch_size=32, epochs=10):\n",
    "    X, y, tokenizer = preprocess_data(df, max_words, max_len)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    model = build_lstm_model(max_words, max_len, embedding_dim)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    val_loss, val_mae = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {val_loss}, Validation MAE: {val_mae}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# 6. 인간 리뷰 학습\n",
    "print(\"Training on Human Reviews...\")\n",
    "human_model, human_tokenizer = train_and_evaluate(human_reviews)\n",
    "\n",
    "# 7. AI 리뷰 학습\n",
    "print(\"Training on AI Reviews...\")\n",
    "ai_model, ai_tokenizer = train_and_evaluate(ai_reviews)\n",
    "\n",
    "# 모델 저장 (선택 사항)\n",
    "# human_model.save('human_review_model.h5')\n",
    "# ai_model.save('ai_review_model.h5')\n",
    "\n",
    "human_model.save('human_review_model.h5', save_format='h5', include_optimizer=True)\n",
    "ai_model.save('ai_review_model.h5', save_format='h5', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5df289-1b5c-454c-85b9-3232298c4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.metrics import MeanSquaredError\n",
    "\n",
    "# 'mse'를 커스텀 오브젝트로 등록\n",
    "custom_objects = {\n",
    "    \"mse\": MeanSquaredError()\n",
    "}\n",
    "\n",
    "# 모델 불러오기\n",
    "human_model = tf.keras.models.load_model('human_review_model.h5', custom_objects=custom_objects)\n",
    "ai_model = tf.keras.models.load_model('ai_review_model.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59477f6-402b-4e9b-ab69-bfca04d4cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\PycharmProjects\\iM_ML-DL\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 13 variables whereas the saved optimizer has 24 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 모델 불러오기\n",
    "human_model = tf.keras.models.load_model('human_review_model_x2.keras')\n",
    "ai_model = tf.keras.models.load_model('ai_review_model_x2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f42e32-bbcc-4831-a46a-57e120fd01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 컴파일\n",
    "human_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "ai_model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee93e7a-5e59-4e4f-beff-a0c7d24e53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb04691-3097-4d0c-a90d-07ad897233e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Human Reviews...\n",
      "Epoch 1/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 245ms/step - loss: 1.6056 - mae: 0.9797 - val_loss: 1.4377 - val_mae: 1.0071\n",
      "Epoch 2/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 292ms/step - loss: 1.4981 - mae: 0.9401 - val_loss: 1.3431 - val_mae: 0.9134\n",
      "Epoch 3/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 282ms/step - loss: 1.5460 - mae: 0.9708 - val_loss: 1.3508 - val_mae: 0.9395\n",
      "Epoch 4/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 262ms/step - loss: 1.4644 - mae: 0.9288 - val_loss: 1.3346 - val_mae: 0.9186\n",
      "Epoch 5/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 279ms/step - loss: 1.4210 - mae: 0.9104 - val_loss: 1.3403 - val_mae: 0.9106\n",
      "Epoch 6/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 305ms/step - loss: 1.4073 - mae: 0.9119 - val_loss: 1.3521 - val_mae: 0.9324\n",
      "Epoch 7/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 487ms/step - loss: 1.4184 - mae: 0.9188 - val_loss: 1.3424 - val_mae: 0.9152\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 255ms/step - loss: 1.4321 - mae: 0.9433\n",
      "Validation Loss: 1.3345632553100586, Validation MAE: 0.9185949563980103\n",
      "Training on AI Reviews...\n",
      "Epoch 1/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 508ms/step - loss: 1.6179 - mae: 0.9902 - val_loss: 1.3200 - val_mae: 0.9342\n",
      "Epoch 2/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 482ms/step - loss: 1.5773 - mae: 0.9737 - val_loss: 1.3084 - val_mae: 0.9228\n",
      "Epoch 3/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 494ms/step - loss: 1.5611 - mae: 0.9690 - val_loss: 1.3083 - val_mae: 0.8869\n",
      "Epoch 4/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 491ms/step - loss: 1.5283 - mae: 0.9594 - val_loss: 1.3182 - val_mae: 0.9293\n",
      "Epoch 5/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 512ms/step - loss: 1.4854 - mae: 0.9510 - val_loss: 1.3252 - val_mae: 0.8958\n",
      "Epoch 6/10\n",
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 537ms/step - loss: 1.4631 - mae: 0.9378 - val_loss: 1.3520 - val_mae: 0.9509\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 181ms/step - loss: 1.2330 - mae: 0.8720\n",
      "Validation Loss: 1.3083401918411255, Validation MAE: 0.8868522644042969\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "data_path = 'fake_reviews_dataset.csv'  # 데이터 파일 경로\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 2. 인간과 AI 리뷰 분리\n",
    "human_reviews = data[data['label'] == 'OR']\n",
    "ai_reviews = data[data['label'] == 'CG']\n",
    "\n",
    "# 3. 데이터 전처리 함수 정의\n",
    "def preprocess_data(df, max_words=10000, max_len=200):\n",
    "    texts = df['text_'].astype(str).values\n",
    "    ratings = df['rating'].values\n",
    "\n",
    "    # 텍스트 토큰화\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "    return padded_sequences, ratings, tokenizer\n",
    "\n",
    "# 4. 모델 정의 함수\n",
    "def build_lstm_model(max_words=10000, max_len=200, embedding_dim=128):\n",
    "    model = Sequential([\n",
    "        Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "        LSTM(128, return_sequences=True, dropout=0.2),\n",
    "        LSTM(64, dropout=0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='linear')  # 회귀 문제\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# 5. 학습 및 평가\n",
    "def train_and_evaluate(df, max_words=10000, max_len=200, embedding_dim=128, test_size=0.2, batch_size=32, epochs=10, model = None):\n",
    "    X, y, tokenizer = preprocess_data(df, max_words, max_len)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # model = build_lstm_model(max_words, max_len, embedding_dim)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    val_loss, val_mae = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {val_loss}, Validation MAE: {val_mae}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# 6. 인간 리뷰 학습\n",
    "print(\"Training on Human Reviews...\")\n",
    "human_model, human_tokenizer = train_and_evaluate(human_reviews, model = human_model)\n",
    "\n",
    "# 7. AI 리뷰 학습\n",
    "print(\"Training on AI Reviews...\")\n",
    "ai_model, ai_tokenizer = train_and_evaluate(ai_reviews, model = ai_model)\n",
    "\n",
    "# 모델 저장 (선택 사항)\n",
    "# human_model.save('human_review_model.h5')\n",
    "# ai_model.save('ai_review_model.h5')\n",
    "\n",
    "human_model.save('human_review_model_x2.keras', include_optimizer=True)\n",
    "ai_model.save('ai_review_model_x2.keras', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d504d0-f547-496f-95d0-750a3476d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_model.save('human_review_model_x2.keras', include_optimizer=True)\n",
    "ai_model.save('ai_review_model_x2.keras', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64448900-53a1-4e55-81e7-a198a9fc74f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
