{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0de9b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82102\\PycharmProjects\\iM_ML-DL\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40432 entries, 0 to 40431\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   category  40432 non-null  object \n",
      " 1   rating    40432 non-null  float64\n",
      " 2   label     40432 non-null  object \n",
      " 3   text_     40432 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. 데이터 확인 및 전처리\n",
    "# Null 값 확인 및 제거\n",
    "print(data.info())\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 텍스트와 라벨 분리\n",
    "texts = data['text_']\n",
    "labels = data['label']\n",
    "\n",
    "# 'CG'를 1로, 'OR'을 0으로 변환\n",
    "labels = labels.map({'CG': 1, 'OR': 0}).astype(np.float32)\n",
    "\n",
    "# 3. 텍스트 데이터 전처리\n",
    "# 토크나이저 정의\n",
    "max_words = 10000  # 사용할 최대 단어 수\n",
    "max_len = 100  # 리뷰의 최대 길이\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 텍스트 시퀀스 변환 및 패딩\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 4. 데이터셋 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. 모델 생성\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7. 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 8. 모델 저장\n",
    "model.save(\"ai_human_review_classifier.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b4327",
   "metadata": {},
   "source": [
    "### 잘린 텍스트 생성하기 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ea3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0453dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (369639, 296), y shape: (369639,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 'CG' 레이블 필터링\n",
    "cg_data = data[data['label'] == 'CG']\n",
    "\n",
    "# 마침표로 끝나지 않는 텍스트 필터링\n",
    "cg_data_complete = cg_data[cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "# 마침표로 끝나는 텍스트만 학습 데이터로 사용\n",
    "cg_data_incomplete = cg_data[~cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(texts):\n",
    "    return [text.lower() for text in texts]\n",
    "\n",
    "# 학습용 데이터 준비\n",
    "texts = preprocess_text(cg_data_complete['text_'].tolist())\n",
    "# texts = cg_data_complete['text_'].tolist()\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 시퀀스 생성\n",
    "input_sequences = []\n",
    "for sentence in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 패딩 처리\n",
    "max_seq_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# 입력(X)과 출력(Y) 분리\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64cb2b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 296, 50)           369700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 296, 64)           29440     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7394)              480610    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 912,774\n",
      "Trainable params: 912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length-1),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    LSTM(units=64),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a722c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1444/1444 [==============================] - 1596s 1s/step - loss: 1.8473 - accuracy: 0.5562\n",
      "Epoch 2/15\n",
      "1444/1444 [==============================] - 1618s 1s/step - loss: 1.8395 - accuracy: 0.5574\n",
      "Epoch 3/15\n",
      "1444/1444 [==============================] - 1569s 1s/step - loss: 1.8314 - accuracy: 0.5588\n",
      "Epoch 4/15\n",
      "1444/1444 [==============================] - 1576s 1s/step - loss: 1.8244 - accuracy: 0.5602\n",
      "Epoch 5/15\n",
      "1444/1444 [==============================] - 1595s 1s/step - loss: 1.8169 - accuracy: 0.5618\n",
      "Epoch 6/15\n",
      "1444/1444 [==============================] - 1617s 1s/step - loss: 1.8108 - accuracy: 0.5630\n",
      "Epoch 7/15\n",
      "1444/1444 [==============================] - 1630s 1s/step - loss: 1.8038 - accuracy: 0.5647\n",
      "Epoch 8/15\n",
      "1444/1444 [==============================] - 1646s 1s/step - loss: 1.7970 - accuracy: 0.5661\n",
      "Epoch 9/15\n",
      "1444/1444 [==============================] - 1669s 1s/step - loss: 1.7908 - accuracy: 0.5674\n",
      "Epoch 10/15\n",
      "1444/1444 [==============================] - 1677s 1s/step - loss: 1.7834 - accuracy: 0.5691\n",
      "Epoch 11/15\n",
      "1444/1444 [==============================] - 1690s 1s/step - loss: 1.7775 - accuracy: 0.5701\n",
      "Epoch 12/15\n",
      "1444/1444 [==============================] - 1701s 1s/step - loss: 1.7716 - accuracy: 0.5714\n",
      "Epoch 13/15\n",
      "1444/1444 [==============================] - 1715s 1s/step - loss: 1.7651 - accuracy: 0.5729\n",
      "Epoch 14/15\n",
      "1444/1444 [==============================] - 1715s 1s/step - loss: 1.7599 - accuracy: 0.5739\n",
      "Epoch 15/15\n",
      "1444/1444 [==============================] - 1705s 1s/step - loss: 1.7534 - accuracy: 0.5749\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 256\n",
    "history = model.fit(X, y, epochs=epochs, batch_size = batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc42c2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Makes may tea with out stirring. The only problem is that it's kind of hard to put\n",
      "Generated Text: Makes may tea with out stirring. The only problem is that it's kind of hard to put the bottom on top the plastic part is also very light and easy to wash i have used it twice\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, seed_text, max_length=20):\n",
    "    for _ in range(max_length):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        # 예측된 값에서 가장 확률이 높은 단어 인덱스 선택\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "        \n",
    "        # 예측된 인덱스를 단어로 변환\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        \n",
    "        # 단어를 시드 텍스트에 추가\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "        # 마침표가 나오면 텍스트 생성 종료\n",
    "        if output_word == '.':\n",
    "            break\n",
    "    \n",
    "    return seed_text\n",
    "\n",
    "# 마침표가 없는 리뷰 중 하나를 가져와 뒷부분 생성\n",
    "test_text = cg_data_incomplete.iloc[10]['text_']\n",
    "generated_text = generate_text(model, tokenizer, test_text)\n",
    "\n",
    "print(f\"Original Text: {test_text}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"recover_model_lstm_x3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a02e18-6266-4682-b9a6-cb111ec6c999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.idea', '.ipynb_checkpoints', 'ai_human_review_classifier.h5', 'ai_review_model.h5', 'ai_review_model_x2.h5', 'ai_review_model_x2.keras', 'AI_text', 'chromedriver.exe', 'fake_reviews_dataset.csv', 'generated_reviews.csv', 'gmarket_reviews.csv', 'human_review_model.h5', 'human_review_model_x2.h5', 'human_review_model_x2.keras', 'LICENSE.chromedriver', 'naver_store.ipynb', 'processed_reviews.csv', 'processed_reviews_test.csv', 'processed_reviews_test_utf8.csv', 'processed_reviews_utf8.csv', 'product_review.csv', 'recover_model_lstm_x25.keras', 'review_test.csv', 'review_train.csv', 'THIRD_PARTY_NOTICES.chromedriver', '딥러닝분석.ipynb', '리뷰구분모델_CNN.ipynb', '리뷰구분모델_LSTM.ipynb', '리뷰구분모델_MLP.ipynb', '리뷰복원_CNN_LSTM.ipynb', '리뷰평점예측_LSTM.ipynb', '어순분석.ipynb', '잘린텍스트복원.ipynb', '잘린텍스트복원_desktop.ipynb', '크롤링.ipynb', '한국어리뷰구분.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c84a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=recover_model_lstm_x25.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecover_model_lstm_x25.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\PycharmProjects\\iM_ML-DL\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=recover_model_lstm_x25.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('recover_model_lstm_x25.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5d795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
