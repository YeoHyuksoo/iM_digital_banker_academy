{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d5a1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\82102\\PycharmProjects\\iM_ML-DL\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40432 entries, 0 to 40431\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   category  40432 non-null  object \n",
      " 1   rating    40432 non-null  float64\n",
      " 2   label     40432 non-null  object \n",
      " 3   text_     40432 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 2. 데이터 확인 및 전처리\n",
    "# Null 값 확인 및 제거\n",
    "print(data.info())\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 텍스트와 라벨 분리\n",
    "texts = data['text_']\n",
    "labels = data['label']\n",
    "\n",
    "# 'CG'를 1로, 'OR'을 0으로 변환\n",
    "labels = labels.map({'CG': 1, 'OR': 0}).astype(np.float32)\n",
    "\n",
    "# 3. 텍스트 데이터 전처리\n",
    "# 토크나이저 정의\n",
    "max_words = 10000  # 사용할 최대 단어 수\n",
    "max_len = 100  # 리뷰의 최대 길이\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# 텍스트 시퀀스 변환 및 패딩\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 4. 데이터셋 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. 모델 생성\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. 모델 학습\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7. 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 8. 모델 저장\n",
    "model.save(\"ai_human_review_classifier.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b96b9",
   "metadata": {},
   "source": [
    "### 잘린 텍스트 생성하기 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca4e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8b26d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (369639, 296), y shape: (369639,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 'CG' 레이블 필터링\n",
    "cg_data = data[data['label'] == 'CG']\n",
    "\n",
    "# 마침표로 끝나지 않는 텍스트 필터링\n",
    "cg_data_complete = cg_data[cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "# 마침표로 끝나는 텍스트만 학습 데이터로 사용\n",
    "cg_data_incomplete = cg_data[~cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(texts):\n",
    "    return [text.lower() for text in texts]\n",
    "\n",
    "# 학습용 데이터 준비\n",
    "texts = preprocess_text(cg_data_complete['text_'].tolist())\n",
    "# texts = cg_data_complete['text_'].tolist()\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 시퀀스 생성\n",
    "input_sequences = []\n",
    "for sentence in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 패딩 처리\n",
    "max_seq_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# 입력(X)과 출력(Y) 분리\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "# y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b63ecb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 296, 50)           369700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 296, 64)           29440     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7394)              480610    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 912,774\n",
      "Trainable params: 912,774\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length-1),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    LSTM(units=64),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1444/1444 [==============================] - 1582s 1s/step - loss: 1.7481 - accuracy: 0.5761\n",
      "Epoch 2/15\n",
      "1441/1444 [============================>.] - ETA: 3s - loss: 1.7421 - accuracy: 0.5775"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "batch_size = 256\n",
    "history = model.fit(X, y, epochs=epochs, batch_size = batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edb90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Makes may tea with out stirring. The only problem is that it's kind of hard to put\n",
      "Generated Text: Makes may tea with out stirring. The only problem is that it's kind of hard to put the bottom on top the plastic part is also very light and easy to wash i have used it twice\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, seed_text, max_length=20):\n",
    "    for _ in range(max_length):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        # 예측된 값에서 가장 확률이 높은 단어 인덱스 선택\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "        \n",
    "        # 예측된 인덱스를 단어로 변환\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        \n",
    "        # 단어를 시드 텍스트에 추가\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "        # 마침표가 나오면 텍스트 생성 종료\n",
    "        if output_word == '.':\n",
    "            break\n",
    "    \n",
    "    return seed_text\n",
    "\n",
    "# 마침표가 없는 리뷰 중 하나를 가져와 뒷부분 생성\n",
    "test_text = cg_data_incomplete.iloc[20]['text_']\n",
    "generated_text = generate_text(model, tokenizer, test_text)\n",
    "\n",
    "print(f\"Original Text: {test_text}\")\n",
    "print(f\"Generated Text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf241c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"recover_model_lstm_x3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237997d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'tensorflow.python.framework.ops' (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecover_model_lstm_x2.5.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:37\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:37\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:150\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Operation\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegisterGradient\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _colocate_with \u001b[38;5;28;01mas\u001b[39;00m colocate_with\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_to_collection\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Tensor' from 'tensorflow.python.framework.ops' (C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('recover_model_lstm_x2.5.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54460abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "X shape: (375930, 297), y shape: (375930,)\n",
      "Epoch 1/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1881s\u001b[0m 640ms/step - accuracy: 0.4568 - loss: 2.3155\n",
      "Epoch 2/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1907s\u001b[0m 649ms/step - accuracy: 0.4614 - loss: 2.2951\n",
      "Epoch 3/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2135s\u001b[0m 727ms/step - accuracy: 0.4657 - loss: 2.2749\n",
      "Epoch 4/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7531s\u001b[0m 3s/step - accuracy: 0.4690 - loss: 2.2575\n",
      "Epoch 5/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5209s\u001b[0m 2s/step - accuracy: 0.4721 - loss: 2.2399\n",
      "Epoch 6/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4031s\u001b[0m 1s/step - accuracy: 0.4726 - loss: 2.2279\n",
      "Epoch 7/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2302s\u001b[0m 784ms/step - accuracy: 0.4770 - loss: 2.2135\n",
      "Epoch 8/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1969s\u001b[0m 670ms/step - accuracy: 0.4809 - loss: 2.1931\n",
      "Epoch 9/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1887s\u001b[0m 642ms/step - accuracy: 0.4826 - loss: 2.1847\n",
      "Epoch 10/10\n",
      "\u001b[1m2937/2937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54763s\u001b[0m 19s/step - accuracy: 0.4846 - loss: 2.1693\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "file_path = \"fake_reviews_dataset.csv\"  # 데이터 파일 경로\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 'CG' 레이블 필터링\n",
    "cg_data = data[data['label'] == 'CG']\n",
    "\n",
    "# 마침표로 끝나지 않는 텍스트 필터링\n",
    "cg_data_complete = cg_data[cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "# 마침표로 끝나는 텍스트만 학습 데이터로 사용\n",
    "cg_data_incomplete = cg_data[~cg_data['text_'].str.endswith(('.', '?', '!'))]\n",
    "\n",
    "# 문장의 끝을 나타내는 특수 토큰 추가\n",
    "end_token = \"<end>\"\n",
    "\n",
    "# 텍스트 전처리 함수\n",
    "def preprocess_text(texts):\n",
    "    return [text.lower() + \" \" + end_token for text in texts]  # 각 문장 끝에 \"<end>\" 추가\n",
    "\n",
    "# 학습용 데이터 준비\n",
    "texts = preprocess_text(cg_data_complete['text_'].tolist())\n",
    "print('.' in texts)\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # vocab_size에 <end> 포함\n",
    "\n",
    "# 시퀀스 생성\n",
    "input_sequences = []\n",
    "for sentence in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 패딩 처리\n",
    "max_seq_length = max(len(x) for x in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# 입력(X)과 출력(Y) 분리\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# 모델 정의\n",
    "# embedding_dim = 50\n",
    "\n",
    "# model = Sequential([\n",
    "#     Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length-1),\n",
    "#     LSTM(units=64, return_sequences=True),\n",
    "#     LSTM(units=64),\n",
    "#     Dense(units=vocab_size, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# 모델 학습\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "490e103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of incomplete reviews:  13925\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: Old movie.  It is a real shame.  Not for children.  It's a sad movie.  Great story and the acting is good.  Great action.  I would recommend it to any person.  I'm a huge fan of the show and I recommend it to anyone who likes action and suspense.  I also love the fact that it is a comedy.  I liked the ending.  I'm sure it could have been better. \n",
      "Generated Text: Old movie.  It is a real shame.  Not for children.  It's a sad movie.  Great story and the acting is good.  Great action.  I would recommend it to any person.  I'm a huge fan of the show and I recommend it to anyone who likes action and suspense.  I also love the fact that it is a comedy.  I liked the ending.  I'm sure it could have been better.  i would recommend this book to anyone who likes action movies i have read all of the books by the\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: This movie is unbelievable.  It has the wit, humor, and heart of a thriller.  It is so heart-warming.  I can't say enough good things about it.  If you like thriller, action, and a bit of magic then you will like this movie.  It is a great movie to watch and will make you laugh and cry.  It has great special effects and great performances by all the actors.  The story is very well\n",
      "Generated Text: This movie is unbelievable.  It has the wit, humor, and heart of a thriller.  It is so heart-warming.  I can't say enough good things about it.  If you like thriller, action, and a bit of magic then you will like this movie.  It is a great movie to watch and will make you laugh and cry.  It has great special effects and great performances by all the actors.  The story is very well told the acting is top notch the movie is very well done the movie is very well done the movie\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: A very discreet movie. Unusual for an action movie.Very good movie.Nice story.Very good.I love this movie! I am a huge fan of Treme. The story is so compelling and well told. The acting is perfect. The story is very well told. The acting is great. The acting is also great. I would recommend this movie to anyone who wants to learn more about the art of making movies.This is an interesting documentary on the\n",
      "Generated Text: A very discreet movie. Unusual for an action movie.Very good movie.Nice story.Very good.I love this movie! I am a huge fan of Treme. The story is so compelling and well told. The acting is perfect. The story is very well told. The acting is great. The acting is also great. I would recommend this movie to anyone who wants to learn more about the art of making movies.This is an interesting documentary on the series by this author this is a great book for a beginner to start it is a good movie i\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: Harrowing, suspenseful, scary, terrifying.  Not a bad movie, just not a lot of action.  If you like suspense and suspense, you will like this movie.I think this is a great movie. It is based on a true story, and it is a very good movie. The acting is great, the storyline is good, and the movie is great. If you like suspense and suspense, you will like this movie.I was happy to\n",
      "Generated Text: Harrowing, suspenseful, scary, terrifying.  Not a bad movie, just not a lot of action.  If you like suspense and suspense, you will like this movie.I think this is a great movie. It is based on a true story, and it is a very good movie. The acting is great, the storyline is good, and the movie is great. If you like suspense and suspense, you will like this movie.I was happy to get a movie that is a good movie but it is a good movie i have read all of the\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: Exit Humanity is a strange, violent, and very disturbing film.  There is a strong sense of self-consciousness about the film.  The film is not a great film, but it is a great film.  I would recommend it.This movie is excellent. The acting is outstanding. The storyline is clever and the directing is brilliant. The story is a masterpiece. I will watch again and again. I highly recommend it to any and all people who are interested\n",
      "Generated Text: Exit Humanity is a strange, violent, and very disturbing film.  There is a strong sense of self-consciousness about the film.  The film is not a great film, but it is a great film.  I would recommend it.This movie is excellent. The acting is outstanding. The storyline is clever and the directing is brilliant. The story is a masterpiece. I will watch again and again. I highly recommend it to any and all people who are interested in the history of the united states is a great book for a beginner to the craft great job i\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: Sorry for the stupid title, it was an interesting show.\n",
      "\n",
      "The story was interesting and interesting.  The acting was good, but the writing was not great.  I would recommend it to anyone who likes action movies.I was excited to see this movie.  The acting was great.  The story was entertaining and I enjoyed the movie too.  I wish I could have seen more of the movie.I was happy to see this film released on DVD, so\n",
      "Generated Text: Sorry for the stupid title, it was an interesting show.\n",
      "\n",
      "The story was interesting and interesting.  The acting was good, but the writing was not great.  I would recommend it to anyone who likes action movies.I was excited to see this movie.  The acting was great.  The story was entertaining and I enjoyed the movie too.  I wish I could have seen more of the movie.I was happy to see this film released on DVD, so it is a good movie i have read all of the books by the author and the first time i\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: The trailers looked pretty fun, and it was an interesting watch.\n",
      "\n",
      "The special effects were pretty good.  The story was interesting, but the acting was not great.  The other two movies were okay, but not good enough to keep it from being a 4 star.\n",
      "\n",
      "Overall, a decent movie with a good story line, good acting and a good story line.I love this movie.  I have watched it many times.  I like that the actors are\n",
      "Generated Text: The trailers looked pretty fun, and it was an interesting watch.\n",
      "\n",
      "The special effects were pretty good.  The story was interesting, but the acting was not great.  The other two movies were okay, but not good enough to keep it from being a 4 star.\n",
      "\n",
      "Overall, a decent movie with a good story line, good acting and a good story line.I love this movie.  I have watched it many times.  I like that the actors are a bit of a good ending i really enjoyed the book i did not like it i liked the ending\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: Brad Pitt continues to pick up steam as the villain in this series.\n",
      "\n",
      "The first season is a very good series and I am glad to see the beginning of the second season. The story is well told and the acting is outstanding. The ending is very disappointing.The series was cancelled by a network after only three seasons. I'm glad to have it back.It was great. I just watched it with my friends. I'm glad they did it.This is a great series\n",
      "Generated Text: Brad Pitt continues to pick up steam as the villain in this series.\n",
      "\n",
      "The first season is a very good series and I am glad to see the beginning of the second season. The story is well told and the acting is outstanding. The ending is very disappointing.The series was cancelled by a network after only three seasons. I'm glad to have it back.It was great. I just watched it with my friends. I'm glad they did it.This is a great series for young people i have read all of the books by the author and the author is a good book\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: I absolutely LOVE this movie!  I love the movie and I will watch it again!  If you like this movie, you will like this movie too!I think this is a great movie. It is based on a true story, and it is a very good movie. The acting is great, the storyline is good, and the movie is great. If you like movies like this, you will like this movie too.This is an interesting documentary on the history of the American west\n",
      "Generated Text: I absolutely LOVE this movie!  I love the movie and I will watch it again!  If you like this movie, you will like this movie too!I think this is a great movie. It is based on a true story, and it is a very good movie. The acting is great, the storyline is good, and the movie is great. If you like movies like this, you will like this movie too.This is an interesting documentary on the history of the American west it is a great read i would recommend this movie to anyone who is looking for a good movie i\n",
      "category:  Movies_and_TV_5\n",
      "Original Text: The original terminator would not be an option.  The two men are now dead.  The only other option would be to get a \"star\" or a \"good\" actress.  If they had this option, they would be able to do it with \"the director.\"  In other words, they could have the most beautiful, talented, and talented actress in the world.  That is what the movie is about.  The only reason I gave it 5 stars is because of\n",
      "Generated Text: The original terminator would not be an option.  The two men are now dead.  The only other option would be to get a \"star\" or a \"good\" actress.  If they had this option, they would be able to do it with \"the director.\"  In other words, they could have the most beautiful, talented, and talented actress in the world.  That is what the movie is about.  The only reason I gave it 5 stars is because of the most common type of cabbage and vegetables like a good story line for a good story and a good\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, seed_text, max_length=20):\n",
    "    for _ in range(max_length):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        # 예측된 값에서 가장 확률이 높은 단어 인덱스 선택\n",
    "        predicted_word_index = np.argmax(predicted, axis=-1)[0]\n",
    "        \n",
    "        # 예측된 인덱스를 단어로 변환\n",
    "        output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
    "        \n",
    "        if output_word == end_token or output_word == 'end':\n",
    "            break\n",
    "        \n",
    "        # 단어를 시드 텍스트에 추가\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "        # 마침표가 나오면 텍스트 생성 종료\n",
    "        if output_word in ['.', '?', '!']:\n",
    "            break\n",
    "    \n",
    "    return seed_text\n",
    "\n",
    "print(\"count of incomplete reviews: \", len(cg_data_incomplete))\n",
    "\n",
    "for i in range(5000, 5010, 1):\n",
    "    # 마침표가 없는 리뷰 중 하나를 가져와 뒷부분 생성\n",
    "    test_text = cg_data_incomplete.iloc[i]['text_']\n",
    "    generated_text = generate_text(model, tokenizer, test_text)\n",
    "\n",
    "    print(\"category: \", cg_data_incomplete.iloc[i]['category'])\n",
    "    print(f\"Original Text: {test_text}\")\n",
    "    print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1d84097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"recover_model_lstm_end_1.33.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af03bf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">369,700</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">29,440</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7394</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">480,610</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m297\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m369,700\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m297\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m29,440\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m33,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7394\u001b[0m)                │         \u001b[38;5;34m480,610\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,738,324</span> (10.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,738,324\u001b[0m (10.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">912,774</span> (3.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m912,774\u001b[0m (3.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,825,550</span> (6.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,825,550\u001b[0m (6.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('recover_model_lstm_end_1.33.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d856ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78e3b7-1b07-4a75-87ee-8572fa422764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
