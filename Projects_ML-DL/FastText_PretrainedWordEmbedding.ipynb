{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b296e8c-7c10-4e93-90f3-5280b86dcd8b",
   "metadata": {},
   "source": [
    "#### FastText 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65266099-1e98-4767-966b-4201c5f2da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebec8a2f-bd23-46c2-8e36-3a1be3b79d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x2aeda6bab60>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\n",
    "                           filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6113a57-fce1-43a9-85f6-2c48262f7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87476611-b827-45b6-b661-a2d7393f5069",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text) #sub는 치환할 때 사용하는 개념, 소괄호로 시작하는 모든 것들을 빈문자로 치환\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f05c42-4052-43ff-b9ea-02b5a2827759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273424"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c12c18-5200-4998-a9ce-43a91d1bd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1dbe44-da64-4864-98b4-473052ca3c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = result, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d72e06-ecc9-4dd1-9193-b7e2005956d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melectrofishing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\NLP\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\PycharmProjects\\NLP\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:518\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    516\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[1;32m--> 518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    521\u001b[0m     mean \u001b[38;5;241m=\u001b[39m mean \u001b[38;5;241m/\u001b[39m total_weight\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7389a00-c0a3-41e5-907d-4bcebfdecbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(result, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815353fe-a59d-40a6-972e-ca6f18beb9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fishing', 0.9186096787452698),\n",
       " ('flushing', 0.8999102115631104),\n",
       " ('flourishing', 0.8978756070137024),\n",
       " ('flashing', 0.8956512808799744),\n",
       " ('vanishing', 0.8939927220344543),\n",
       " ('ingrid', 0.8936792612075806),\n",
       " ('licensing', 0.8930103778839111),\n",
       " ('smashing', 0.8919094800949097),\n",
       " ('transplanting', 0.8912322521209717),\n",
       " ('refreshing', 0.8889642953872681)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cbab2-d88c-4f44-b31b-e6a4bb821d4d",
   "metadata": {},
   "source": [
    "### 한국어 FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a049d4d-44d9-4fa3-8cf5-d16c0afde012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import hgtk\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef4dcb17-d8c4-4126-bc3e-d9f0645ecf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_total.txt', <http.client.HTTPMessage at 0x2ae99d02fe0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\",\n",
    "                           filename=\"ratings_total.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e29439-8b4a-4d20-90f3-03fa58be4270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 리뷰 개수 :  200000\n"
     ]
    }
   ],
   "source": [
    "total_data = pd.read_table(\"ratings_total.txt\", names = ['ratings', 'reviews'])\n",
    "print(\"전체 리뷰 개수 : \", len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "279b3edc-0053-46ec-9013-84eac6b7602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글인지 체크\n",
    "hgtk.checker.is_hangul('ㄱ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f63aff71-4b17-4ee2-a472-fa5bf43849c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgtk.checker.is_hangul('28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d72857-331a-444e-9c22-2a3c0644ccb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ㄴ', 'ㅏ', 'ㅁ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 음절을 초성, 중성, 종성으로 분해\n",
    "hgtk.letter.decompose('남')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b534b2a5-12a4-405c-b2e3-7853a704e29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초성, 중성을 결합\n",
    "hgtk.letter.compose('ㄴ', 'ㅏ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e642d92a-2037-441f-9d4b-058b551dd564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'남'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초성, 중성, 종성을 결합\n",
    "hgtk.letter.compose('ㄴ', 'ㅏ', 'ㅁ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce7ca5e-fec0-4f95-ae0a-d87a73117bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotHangulException",
     "evalue": "No valid Hangul character index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\PycharmProjects\\NLP\\.venv\\lib\\site-packages\\hgtk\\letter.py:27\u001b[0m, in \u001b[0;36mcompose\u001b[1;34m(chosung, joongsung, jongsung)\u001b[0m\n\u001b[0;32m     26\u001b[0m chosung_index \u001b[38;5;241m=\u001b[39m CHO\u001b[38;5;241m.\u001b[39mindex(chosung)\n\u001b[1;32m---> 27\u001b[0m joongsung_index \u001b[38;5;241m=\u001b[39m \u001b[43mJOONG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoongsung\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m jongsung_index \u001b[38;5;241m=\u001b[39m JONG\u001b[38;5;241m.\u001b[39mindex(jongsung)\n",
      "\u001b[1;31mValueError\u001b[0m: tuple.index(x): x not in tuple",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotHangulException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 결합할 수 없는 상황에서는 에러 발생\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mhgtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mletter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mㄴ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mㅁ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mㅁ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\NLP\\.venv\\lib\\site-packages\\hgtk\\letter.py:30\u001b[0m, in \u001b[0;36mcompose\u001b[1;34m(chosung, joongsung, jongsung)\u001b[0m\n\u001b[0;32m     28\u001b[0m     jongsung_index \u001b[38;5;241m=\u001b[39m JONG\u001b[38;5;241m.\u001b[39mindex(jongsung)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotHangulException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo valid Hangul character index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unichr(\u001b[38;5;241m0xAC00\u001b[39m \u001b[38;5;241m+\u001b[39m chosung_index \u001b[38;5;241m*\u001b[39m NUM_JOONG \u001b[38;5;241m*\u001b[39m NUM_JONG \u001b[38;5;241m+\u001b[39m joongsung_index \u001b[38;5;241m*\u001b[39m NUM_JONG \u001b[38;5;241m+\u001b[39m jongsung_index)\n",
      "\u001b[1;31mNotHangulException\u001b[0m: No valid Hangul character index"
     ]
    }
   ],
   "source": [
    "# 결합할 수 없는 상황에서는 에러 발생\n",
    "hgtk.letter.compose('ㄴ', 'ㅁ', 'ㅁ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6af23a4-ee7c-474f-a2ff-aa70e956d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_jamo(token):\n",
    "    def to_special_token(jamo):\n",
    "        if not jamo:\n",
    "            return '-'\n",
    "        else:\n",
    "            return jamo\n",
    "\n",
    "    decomposed_token = ''\n",
    "    for char in token:\n",
    "        try:\n",
    "            # char(음절)을 초성, 중성, 종성으로 분리\n",
    "            cho, jung, jong = hgtk.letter.decompose(char)\n",
    "\n",
    "            # 자모가 빈 문자일 경우 특수문자 -로 대체\n",
    "            cho = to_special_token(cho)\n",
    "            jung = to_special_token(jung)\n",
    "            jong = to_special_token(jong)\n",
    "            decomposed_token = decomposed_token + cho + jung + jong\n",
    "\n",
    "        # 만약 char(음절)이 한글이 아닐 경우 자모를 나누지 않고 추가\n",
    "        except Exception as exception:\n",
    "            if type(exception).__name__ == 'NotHangulException':\n",
    "                decomposed_token += char\n",
    "\n",
    "    # 단어 토큰의 자모 단위 분리 결과를 추가\n",
    "    return decomposed_token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4f980a2-e01b-40b4-a8e7-c13214ccbcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_jamo('남동생')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84ee3bd6-ec1d-4790-8c1f-f5f6e0870928",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab(r'C:\\mecab\\mecab-ko-dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ee3cc8f-146e-414b-835d-32c19cfdf1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['선물', '용', '으로', '빨리', '받', '아서', '전달', '했어야', '하', '는', '상품', '이', '었', '는데', '머그', '컵', '만', '와서', '당황', '했', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(mecab.morphs('선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea1463ad-2d7c-4338-9070-569dbe78ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㅅㅓㄴㅁㅜㄹ', 'ㅇㅛㅇ', 'ㅇㅡ-ㄹㅗ-', 'ㅃㅏㄹㄹㅣ-', 'ㅂㅏㄷ', 'ㅇㅏ-ㅅㅓ-', 'ㅈㅓㄴㄷㅏㄹ', 'ㅎㅐㅆㅇㅓ-ㅇㅑ-', 'ㅎㅏ-', 'ㄴㅡㄴ', 'ㅅㅏㅇㅍㅜㅁ', 'ㅇㅣ-', 'ㅇㅓㅆ', 'ㄴㅡㄴㄷㅔ-', 'ㅁㅓ-ㄱㅡ-', 'ㅋㅓㅂ', 'ㅁㅏㄴ', 'ㅇㅘ-ㅅㅓ-', 'ㄷㅏㅇㅎㅘㅇ', 'ㅎㅐㅆ', 'ㅅㅡㅂㄴㅣ-ㄷㅏ-', '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_by_jamo(s):\n",
    "    return [word_to_jamo(token) for token in mecab.morphs(s)]\n",
    "\n",
    "print(tokenize_by_jamo('선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d1568a7-5d47-4a88-97c8-fa706d7edb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_data = []\n",
    "\n",
    "for sample in total_data['reviews'].to_list():\n",
    "    tokenzied_sample = tokenize_by_jamo(sample) # 자소 단위 토큰화\n",
    "    tokenized_data.append(tokenzied_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d558fe1f-b986-4203-bce6-6fea9dd41862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅂㅐ-ㄱㅗㅇ', 'ㅃㅏ-ㄹㅡ-', 'ㄱㅗ-', 'ㄱㅜㅅ']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13041316-7b40-4384-b2a9-39e1da7a9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jamo_to_word(jamo_sequence):\n",
    "    tokenized_jamo = []\n",
    "    index = 0\n",
    "\n",
    "    # 1. 초기 입력\n",
    "    # jamo_sequence = 'ㄴㅏㅁㄷㅗㅇㅅㅐㅇ'\n",
    "\n",
    "    while index < len(jamo_sequence):\n",
    "        # 문자가 한글(정상적인 자모)이 아닐 경우\n",
    "        if not hgtk.checker.is_hangul(jamo_sequence[index]):\n",
    "            tokenized_jamo.append(jamo_sequence[index])\n",
    "            index = index + 1\n",
    "\n",
    "        # 문자가 정상적인 자모라면 초성, 중성, 종성을 하나의 토큰으로 간주.\n",
    "        else:\n",
    "            tokenized_jamo.append(jamo_sequence[index:index + 3])\n",
    "            index = index + 3\n",
    "\n",
    "    # 2. 자모 단위 토큰화 완료\n",
    "    # tokenized_jamo : ['ㄴㅏㅁ', 'ㄷㅗㅇ', 'ㅅㅐㅇ']\n",
    "\n",
    "    word = ''\n",
    "    try:\n",
    "        for jamo in tokenized_jamo:\n",
    "            # 초성, 중성, 종성의 묶음으로 추정되는 경우\n",
    "            if len(jamo) == 3:\n",
    "                if jamo[2] == \"-\":\n",
    "                    # 종성이 존재하지 않는 경우\n",
    "                    word = word + hgtk.letter.compose(jamo[0], jamo[1])\n",
    "                else:\n",
    "                    # 종성이 존재하는 경우\n",
    "                    word = word + hgtk.letter.compose(jamo[0], jamo[1], jamo[2])\n",
    "            # 한글이 아닌 경우\n",
    "            else:\n",
    "                word = word + jamo\n",
    "\n",
    "    # 복원 중(hgtk.letter.compose) 에러 발생 시 초기 입력 리턴.\n",
    "    # 복원이 불가능한 경우 예시) 'ㄴ!ㅁㄷㅗㅇㅅㅐㅇ'\n",
    "    except Exception as exception:\n",
    "        if type(exception).__name__ == 'NotHangulException':\n",
    "            return jamo_sequence\n",
    "\n",
    "    # 3. 단어로 복원 완료\n",
    "    # word : '남동생'\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a213c8c5-91e6-45f7-92bb-66a4ff0352b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'남동생'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jamo_to_word('ㄴㅏㅁㄷㅗㅇㅅㅐㅇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "686e92bf-7a1d-4fbb-91b8-b890071a6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bece7ec9-461c-4db3-87d3-1a8eaa118f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [00:02<00:00, 90424.94 line/s]\n"
     ]
    }
   ],
   "source": [
    "with open('tokenized_data.txt', 'w', encoding = 'utf-8') as out:\n",
    "    for line in tqdm(tokenized_data, unit=' line'):\n",
    "        out.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac80a8ba-91f9-4dfc-843e-a08dabf4e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised('tokenized_data.txt', model='cbow')\n",
    "model.save_model(\"fasttext.bin\") # 모델 저장\n",
    "model = fasttext.load_model(\"fasttext.bin\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7197bde-ee9e-429c-97b7-8f23e2e7b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27820098,  0.01429991,  0.31213486, -0.35917503, -0.21511072,\n",
       "       -0.9479429 , -0.43248788,  0.6337056 ,  0.07769312,  0.04124201,\n",
       "       -0.44832966, -0.06288446,  0.17858742,  0.36418223,  0.0443569 ,\n",
       "        0.7182074 ,  0.11097914,  0.88904524,  0.04093075, -0.0245559 ,\n",
       "        0.37597132,  0.19367361, -0.5170682 , -0.29743513,  0.9607574 ,\n",
       "        0.51476693, -0.0248489 ,  0.54350096,  0.50881547, -0.544389  ,\n",
       "       -0.1345461 ,  0.31495535,  0.7024569 , -0.89429885,  0.5910413 ,\n",
       "        1.1590027 , -0.1177251 ,  0.34002963, -0.01661343,  0.22770078,\n",
       "       -0.1731957 , -0.22914377, -0.24304502,  0.274633  ,  0.45931977,\n",
       "        0.26006564, -0.12765561, -0.24375473,  0.33554888,  0.48846737,\n",
       "       -0.18786666, -0.29633668, -0.43678093,  0.8253616 ,  0.92256624,\n",
       "       -0.808224  ,  0.01936796, -0.18253934, -0.0726556 , -0.64245796,\n",
       "        0.0465894 , -0.18543795,  0.36493558, -0.35149637, -0.02741783,\n",
       "        0.4495129 , -0.8531769 ,  0.11590157, -0.22333945, -0.17828614,\n",
       "       -1.2424068 , -0.11992121, -1.3607949 , -1.4432429 ,  0.10225967,\n",
       "        1.2056215 ,  0.47956052, -0.45813632,  0.13667135, -0.20974885,\n",
       "        0.572174  , -0.17543513,  0.30417883,  0.36440736,  0.19429557,\n",
       "       -0.50466466, -0.7531615 ,  0.6459388 ,  0.2870863 , -0.01026244,\n",
       "        1.6231608 ,  0.7850108 , -0.9014219 ,  0.02430859,  0.46156386,\n",
       "        0.3243515 , -0.01336978, -0.91196203, -0.60392785,  0.6722628 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[word_to_jamo('남동생')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "968cbec6-eb2f-48aa-9742-a92fdf0754ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8953864574432373, 'ㄷㅗㅇㅅㅐㅇ'),\n",
       " (0.8176509141921997, 'ㄴㅏㅁㅊㅣㄴ'),\n",
       " (0.7767713069915771, 'ㄴㅏㅁㅍㅕㄴ'),\n",
       " (0.7520563006401062, 'ㅊㅣㄴㄱㅜ-'),\n",
       " (0.731107771396637, 'ㄴㅏㅁㅇㅏ-'),\n",
       " (0.7173229455947876, 'ㅅㅐㅇㅇㅣㄹ'),\n",
       " (0.6996015906333923, 'ㅈㅗ-ㅋㅏ-'),\n",
       " (0.69289231300354, 'ㄸㅏㄹ'),\n",
       " (0.6792193651199341, 'ㄴㅏㄴㅅㅐㅇ'),\n",
       " (0.6789605021476746, 'ㄴㅏㅁㅁㅐ-')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors(word_to_jamo('남동생'), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40e37931-c6e5-4f65-af2c-397b4dce3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(word_sequence):\n",
    "    return [(jamo_to_word(word), similarity) for (similarity, word) in word_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "191fd59d-ac83-406a-a2a3-03191c8fd8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('동생', 0.8953864574432373), ('남친', 0.8176509141921997), ('남편', 0.7767713069915771), ('친구', 0.7520563006401062), ('남아', 0.731107771396637), ('생일', 0.7173229455947876), ('조카', 0.6996015906333923), ('딸', 0.69289231300354), ('난생', 0.6792193651199341), ('남매', 0.6789605021476746)]\n"
     ]
    }
   ],
   "source": [
    "print(transform(model.get_nearest_neighbors(word_to_jamo('남동생'), k=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8522ff0-765c-4839-a115-0f22dda1772f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      3\u001b[0m input_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m----> 5\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedding\u001b[49m(vocab_size, output_dim, input_length\u001b[38;5;241m=\u001b[39minput_length)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Embedding' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "output_dim = 128\n",
    "input_length = 500\n",
    "\n",
    "v = Embedding(vocab_size, output_dim, input_length=input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73cd4058-0a75-46c3-96c8-8bf0be7ecea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86d123fa-4e39-448b-947c-7468c1077aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : 16\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
    "print('단어 집합 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2327a901-7500-4e45-943a-595a64d0a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print('정수 인코딩 결과 :',X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d978adb2-9a42-4ad2-9263-c2220e65cc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print('최대 길이 :',max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e51fd71-301b-41b1-91b9-cf4298370701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과 :\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print('패딩 결과 :')\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63beed29-2afd-492b-af5b-df3ec9e74e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\PycharmProjects\\NLP\\.venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 4s - 4s/step - acc: 0.2857 - loss: 0.6950\n",
      "Epoch 2/100\n",
      "1/1 - 0s - 257ms/step - acc: 0.2857 - loss: 0.6932\n",
      "Epoch 3/100\n",
      "1/1 - 0s - 232ms/step - acc: 0.2857 - loss: 0.6915\n",
      "Epoch 4/100\n",
      "1/1 - 0s - 178ms/step - acc: 0.5714 - loss: 0.6897\n",
      "Epoch 5/100\n",
      "1/1 - 0s - 306ms/step - acc: 0.5714 - loss: 0.6880\n",
      "Epoch 6/100\n",
      "1/1 - 0s - 265ms/step - acc: 0.5714 - loss: 0.6862\n",
      "Epoch 7/100\n",
      "1/1 - 0s - 238ms/step - acc: 0.7143 - loss: 0.6845\n",
      "Epoch 8/100\n",
      "1/1 - 0s - 234ms/step - acc: 0.7143 - loss: 0.6828\n",
      "Epoch 9/100\n",
      "1/1 - 0s - 234ms/step - acc: 0.7143 - loss: 0.6810\n",
      "Epoch 10/100\n",
      "1/1 - 0s - 190ms/step - acc: 0.7143 - loss: 0.6793\n",
      "Epoch 11/100\n",
      "1/1 - 0s - 134ms/step - acc: 0.7143 - loss: 0.6776\n",
      "Epoch 12/100\n",
      "1/1 - 0s - 151ms/step - acc: 0.7143 - loss: 0.6758\n",
      "Epoch 13/100\n",
      "1/1 - 0s - 186ms/step - acc: 0.7143 - loss: 0.6741\n",
      "Epoch 14/100\n",
      "1/1 - 0s - 184ms/step - acc: 0.7143 - loss: 0.6723\n",
      "Epoch 15/100\n",
      "1/1 - 0s - 169ms/step - acc: 0.7143 - loss: 0.6706\n",
      "Epoch 16/100\n",
      "1/1 - 0s - 151ms/step - acc: 0.8571 - loss: 0.6689\n",
      "Epoch 17/100\n",
      "1/1 - 0s - 108ms/step - acc: 0.8571 - loss: 0.6671\n",
      "Epoch 18/100\n",
      "1/1 - 0s - 97ms/step - acc: 0.8571 - loss: 0.6654\n",
      "Epoch 19/100\n",
      "1/1 - 0s - 124ms/step - acc: 0.8571 - loss: 0.6636\n",
      "Epoch 20/100\n",
      "1/1 - 0s - 131ms/step - acc: 0.8571 - loss: 0.6619\n",
      "Epoch 21/100\n",
      "1/1 - 0s - 219ms/step - acc: 1.0000 - loss: 0.6601\n",
      "Epoch 22/100\n",
      "1/1 - 0s - 165ms/step - acc: 1.0000 - loss: 0.6584\n",
      "Epoch 23/100\n",
      "1/1 - 0s - 147ms/step - acc: 1.0000 - loss: 0.6566\n",
      "Epoch 24/100\n",
      "1/1 - 0s - 132ms/step - acc: 1.0000 - loss: 0.6548\n",
      "Epoch 25/100\n",
      "1/1 - 0s - 143ms/step - acc: 1.0000 - loss: 0.6531\n",
      "Epoch 26/100\n",
      "1/1 - 0s - 170ms/step - acc: 1.0000 - loss: 0.6513\n",
      "Epoch 27/100\n",
      "1/1 - 0s - 155ms/step - acc: 1.0000 - loss: 0.6495\n",
      "Epoch 28/100\n",
      "1/1 - 0s - 146ms/step - acc: 1.0000 - loss: 0.6477\n",
      "Epoch 29/100\n",
      "1/1 - 0s - 155ms/step - acc: 1.0000 - loss: 0.6460\n",
      "Epoch 30/100\n",
      "1/1 - 0s - 131ms/step - acc: 1.0000 - loss: 0.6442\n",
      "Epoch 31/100\n",
      "1/1 - 0s - 215ms/step - acc: 1.0000 - loss: 0.6424\n",
      "Epoch 32/100\n",
      "1/1 - 0s - 240ms/step - acc: 1.0000 - loss: 0.6406\n",
      "Epoch 33/100\n",
      "1/1 - 0s - 206ms/step - acc: 1.0000 - loss: 0.6388\n",
      "Epoch 34/100\n",
      "1/1 - 0s - 145ms/step - acc: 1.0000 - loss: 0.6369\n",
      "Epoch 35/100\n",
      "1/1 - 0s - 140ms/step - acc: 1.0000 - loss: 0.6351\n",
      "Epoch 36/100\n",
      "1/1 - 0s - 165ms/step - acc: 1.0000 - loss: 0.6333\n",
      "Epoch 37/100\n",
      "1/1 - 0s - 201ms/step - acc: 1.0000 - loss: 0.6315\n",
      "Epoch 38/100\n",
      "1/1 - 0s - 230ms/step - acc: 1.0000 - loss: 0.6296\n",
      "Epoch 39/100\n",
      "1/1 - 0s - 236ms/step - acc: 1.0000 - loss: 0.6278\n",
      "Epoch 40/100\n",
      "1/1 - 0s - 235ms/step - acc: 1.0000 - loss: 0.6259\n",
      "Epoch 41/100\n",
      "1/1 - 0s - 211ms/step - acc: 1.0000 - loss: 0.6241\n",
      "Epoch 42/100\n",
      "1/1 - 0s - 183ms/step - acc: 1.0000 - loss: 0.6222\n",
      "Epoch 43/100\n",
      "1/1 - 0s - 196ms/step - acc: 1.0000 - loss: 0.6204\n",
      "Epoch 44/100\n",
      "1/1 - 0s - 164ms/step - acc: 1.0000 - loss: 0.6185\n",
      "Epoch 45/100\n",
      "1/1 - 0s - 160ms/step - acc: 1.0000 - loss: 0.6166\n",
      "Epoch 46/100\n",
      "1/1 - 0s - 193ms/step - acc: 1.0000 - loss: 0.6147\n",
      "Epoch 47/100\n",
      "1/1 - 0s - 337ms/step - acc: 1.0000 - loss: 0.6128\n",
      "Epoch 48/100\n",
      "1/1 - 0s - 204ms/step - acc: 1.0000 - loss: 0.6109\n",
      "Epoch 49/100\n",
      "1/1 - 0s - 156ms/step - acc: 1.0000 - loss: 0.6090\n",
      "Epoch 50/100\n",
      "1/1 - 0s - 127ms/step - acc: 1.0000 - loss: 0.6071\n",
      "Epoch 51/100\n",
      "1/1 - 0s - 145ms/step - acc: 1.0000 - loss: 0.6052\n",
      "Epoch 52/100\n",
      "1/1 - 0s - 152ms/step - acc: 1.0000 - loss: 0.6033\n",
      "Epoch 53/100\n",
      "1/1 - 0s - 171ms/step - acc: 1.0000 - loss: 0.6013\n",
      "Epoch 54/100\n",
      "1/1 - 0s - 194ms/step - acc: 1.0000 - loss: 0.5994\n",
      "Epoch 55/100\n",
      "1/1 - 0s - 96ms/step - acc: 1.0000 - loss: 0.5974\n",
      "Epoch 56/100\n",
      "1/1 - 0s - 101ms/step - acc: 1.0000 - loss: 0.5955\n",
      "Epoch 57/100\n",
      "1/1 - 0s - 237ms/step - acc: 1.0000 - loss: 0.5935\n",
      "Epoch 58/100\n",
      "1/1 - 0s - 199ms/step - acc: 1.0000 - loss: 0.5916\n",
      "Epoch 59/100\n",
      "1/1 - 0s - 197ms/step - acc: 1.0000 - loss: 0.5896\n",
      "Epoch 60/100\n",
      "1/1 - 0s - 204ms/step - acc: 1.0000 - loss: 0.5876\n",
      "Epoch 61/100\n",
      "1/1 - 0s - 232ms/step - acc: 1.0000 - loss: 0.5856\n",
      "Epoch 62/100\n",
      "1/1 - 0s - 243ms/step - acc: 1.0000 - loss: 0.5836\n",
      "Epoch 63/100\n",
      "1/1 - 0s - 211ms/step - acc: 1.0000 - loss: 0.5817\n",
      "Epoch 64/100\n",
      "1/1 - 0s - 326ms/step - acc: 1.0000 - loss: 0.5797\n",
      "Epoch 65/100\n",
      "1/1 - 0s - 225ms/step - acc: 1.0000 - loss: 0.5776\n",
      "Epoch 66/100\n",
      "1/1 - 0s - 227ms/step - acc: 1.0000 - loss: 0.5756\n",
      "Epoch 67/100\n",
      "1/1 - 0s - 243ms/step - acc: 1.0000 - loss: 0.5736\n",
      "Epoch 68/100\n",
      "1/1 - 0s - 218ms/step - acc: 1.0000 - loss: 0.5716\n",
      "Epoch 69/100\n",
      "1/1 - 0s - 182ms/step - acc: 1.0000 - loss: 0.5696\n",
      "Epoch 70/100\n",
      "1/1 - 0s - 166ms/step - acc: 1.0000 - loss: 0.5675\n",
      "Epoch 71/100\n",
      "1/1 - 0s - 171ms/step - acc: 1.0000 - loss: 0.5655\n",
      "Epoch 72/100\n",
      "1/1 - 0s - 198ms/step - acc: 1.0000 - loss: 0.5635\n",
      "Epoch 73/100\n",
      "1/1 - 0s - 258ms/step - acc: 1.0000 - loss: 0.5614\n",
      "Epoch 74/100\n",
      "1/1 - 0s - 146ms/step - acc: 1.0000 - loss: 0.5594\n",
      "Epoch 75/100\n",
      "1/1 - 0s - 209ms/step - acc: 1.0000 - loss: 0.5573\n",
      "Epoch 76/100\n",
      "1/1 - 0s - 248ms/step - acc: 1.0000 - loss: 0.5553\n",
      "Epoch 77/100\n",
      "1/1 - 0s - 237ms/step - acc: 1.0000 - loss: 0.5532\n",
      "Epoch 78/100\n",
      "1/1 - 0s - 281ms/step - acc: 1.0000 - loss: 0.5511\n",
      "Epoch 79/100\n",
      "1/1 - 0s - 252ms/step - acc: 1.0000 - loss: 0.5491\n",
      "Epoch 80/100\n",
      "1/1 - 0s - 194ms/step - acc: 1.0000 - loss: 0.5470\n",
      "Epoch 81/100\n",
      "1/1 - 0s - 234ms/step - acc: 1.0000 - loss: 0.5449\n",
      "Epoch 82/100\n",
      "1/1 - 0s - 232ms/step - acc: 1.0000 - loss: 0.5428\n",
      "Epoch 83/100\n",
      "1/1 - 0s - 335ms/step - acc: 1.0000 - loss: 0.5407\n",
      "Epoch 84/100\n",
      "1/1 - 0s - 229ms/step - acc: 1.0000 - loss: 0.5387\n",
      "Epoch 85/100\n",
      "1/1 - 0s - 237ms/step - acc: 1.0000 - loss: 0.5366\n",
      "Epoch 86/100\n",
      "1/1 - 0s - 304ms/step - acc: 1.0000 - loss: 0.5345\n",
      "Epoch 87/100\n",
      "1/1 - 0s - 165ms/step - acc: 1.0000 - loss: 0.5324\n",
      "Epoch 88/100\n",
      "1/1 - 0s - 234ms/step - acc: 1.0000 - loss: 0.5303\n",
      "Epoch 89/100\n",
      "1/1 - 0s - 214ms/step - acc: 1.0000 - loss: 0.5282\n",
      "Epoch 90/100\n",
      "1/1 - 0s - 245ms/step - acc: 1.0000 - loss: 0.5261\n",
      "Epoch 91/100\n",
      "1/1 - 0s - 201ms/step - acc: 1.0000 - loss: 0.5240\n",
      "Epoch 92/100\n",
      "1/1 - 0s - 237ms/step - acc: 1.0000 - loss: 0.5219\n",
      "Epoch 93/100\n",
      "1/1 - 0s - 235ms/step - acc: 1.0000 - loss: 0.5197\n",
      "Epoch 94/100\n",
      "1/1 - 0s - 221ms/step - acc: 1.0000 - loss: 0.5176\n",
      "Epoch 95/100\n",
      "1/1 - 0s - 240ms/step - acc: 1.0000 - loss: 0.5155\n",
      "Epoch 96/100\n",
      "1/1 - 0s - 203ms/step - acc: 1.0000 - loss: 0.5134\n",
      "Epoch 97/100\n",
      "1/1 - 0s - 207ms/step - acc: 1.0000 - loss: 0.5113\n",
      "Epoch 98/100\n",
      "1/1 - 0s - 225ms/step - acc: 1.0000 - loss: 0.5092\n",
      "Epoch 99/100\n",
      "1/1 - 0s - 201ms/step - acc: 1.0000 - loss: 0.5070\n",
      "Epoch 100/100\n",
      "1/1 - 0s - 220ms/step - acc: 1.0000 - loss: 0.5049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2af085edcc0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "embedding_dim = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ff3ca-f7f9-4714-a140-b46f74a01bb8",
   "metadata": {},
   "source": [
    "### 사전훈련된 워드임베딩(Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d5b5fd-d4f3-4d4e-947d-ebaee51531a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 크기(shape) : (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "print('모델의 크기(shape) :', word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c3f0d8-0164-486e-9a11-941833c0dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b782a9c9-053d-4e6a-a0ec-41bf729dd94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 : 16\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1 # 패딩을 고려하여 +1\n",
    "print('단어 집합 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f89382c-92e7-4465-8007-ed51d3142cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행렬의 크기(shape) :  (16, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "print(\"임베딩 행렬의 크기(shape) : \", np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d530dd4-9bae-44cc-bb0a-297f9ba4ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "337486a3-7268-4371-a23f-19d4e1084ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    vector_value = get_vector(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc26eb7-33ea-43f0-91fb-0e0d99b973cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
      "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
      " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
      " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
      "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
      " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
      "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
      "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
      "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
      " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
      " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
      " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
      "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
      "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
      " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
      "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
      "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
      " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
      " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
      "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
      "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
      " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
      "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
      " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
      " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
      "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
      "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
      "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
      " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
      "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
      " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
      " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
      "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
      " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
      "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
      " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
      "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
      " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
      " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
      "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
      "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
      " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
      "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
      " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
      "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
      " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
      "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
      " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
      " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
      " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model['nice'])\n",
    "print(len(word2vec_model['nice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0f99bd-2595-43f6-bf27-79c86ec13b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 nice의 맵핑된 정수 :  1\n"
     ]
    }
   ],
   "source": [
    "print(\"단어 nice의 맵핑된 정수 : \", tokenizer.word_index['nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "821ce90a-77eb-46f2-bd28-8ff0e5db4de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
      "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
      " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
      " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
      "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
      " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
      "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
      "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
      "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
      " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
      " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
      " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
      "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
      "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
      " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
      "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
      "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
      " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
      " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
      "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
      "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
      " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
      "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
      " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
      " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
      "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
      "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
      "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
      " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
      "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
      " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
      " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
      "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
      " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
      "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
      " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
      "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
      " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
      " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
      "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
      "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
      " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
      "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
      " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
      "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
      " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
      "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
      " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
      " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
      " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f258b3c5-8e36-40e5-a296-849d507212c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 결과 : [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n",
      "최대 길이 : 4\n"
     ]
    }
   ],
   "source": [
    "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print('정수 인코딩 결과 :',X_encoded)\n",
    "\n",
    "max_len = max(len(l) for l in X_encoded)\n",
    "print('최대 길이 :',max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1cf4b38-a8d7-47c5-89e1-22f89ea0b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과 :\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print('패딩 결과 :')\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4923eb8a-d388-4247-8ff3-909ee2e26c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 1s - 1s/step - acc: 0.7143 - loss: 0.6504\n",
      "Epoch 2/100\n",
      "1/1 - 0s - 85ms/step - acc: 0.8571 - loss: 0.6327\n",
      "Epoch 3/100\n",
      "1/1 - 0s - 86ms/step - acc: 0.8571 - loss: 0.6156\n",
      "Epoch 4/100\n",
      "1/1 - 0s - 98ms/step - acc: 0.8571 - loss: 0.5990\n",
      "Epoch 5/100\n",
      "1/1 - 0s - 91ms/step - acc: 0.8571 - loss: 0.5830\n",
      "Epoch 6/100\n",
      "1/1 - 0s - 89ms/step - acc: 0.8571 - loss: 0.5675\n",
      "Epoch 7/100\n",
      "1/1 - 0s - 81ms/step - acc: 0.8571 - loss: 0.5525\n",
      "Epoch 8/100\n",
      "1/1 - 0s - 105ms/step - acc: 0.8571 - loss: 0.5380\n",
      "Epoch 9/100\n",
      "1/1 - 0s - 95ms/step - acc: 0.8571 - loss: 0.5240\n",
      "Epoch 10/100\n",
      "1/1 - 0s - 85ms/step - acc: 0.8571 - loss: 0.5105\n",
      "Epoch 11/100\n",
      "1/1 - 0s - 94ms/step - acc: 1.0000 - loss: 0.4974\n",
      "Epoch 12/100\n",
      "1/1 - 0s - 86ms/step - acc: 1.0000 - loss: 0.4848\n",
      "Epoch 13/100\n",
      "1/1 - 0s - 82ms/step - acc: 1.0000 - loss: 0.4726\n",
      "Epoch 14/100\n",
      "1/1 - 0s - 92ms/step - acc: 1.0000 - loss: 0.4608\n",
      "Epoch 15/100\n",
      "1/1 - 0s - 86ms/step - acc: 1.0000 - loss: 0.4495\n",
      "Epoch 16/100\n",
      "1/1 - 0s - 65ms/step - acc: 1.0000 - loss: 0.4385\n",
      "Epoch 17/100\n",
      "1/1 - 0s - 89ms/step - acc: 1.0000 - loss: 0.4278\n",
      "Epoch 18/100\n",
      "1/1 - 0s - 91ms/step - acc: 1.0000 - loss: 0.4176\n",
      "Epoch 19/100\n",
      "1/1 - 0s - 89ms/step - acc: 1.0000 - loss: 0.4076\n",
      "Epoch 20/100\n",
      "1/1 - 0s - 111ms/step - acc: 1.0000 - loss: 0.3980\n",
      "Epoch 21/100\n",
      "1/1 - 0s - 81ms/step - acc: 1.0000 - loss: 0.3887\n",
      "Epoch 22/100\n",
      "1/1 - 0s - 96ms/step - acc: 1.0000 - loss: 0.3796\n",
      "Epoch 23/100\n",
      "1/1 - 0s - 94ms/step - acc: 1.0000 - loss: 0.3709\n",
      "Epoch 24/100\n",
      "1/1 - 0s - 85ms/step - acc: 1.0000 - loss: 0.3624\n",
      "Epoch 25/100\n",
      "1/1 - 0s - 115ms/step - acc: 1.0000 - loss: 0.3542\n",
      "Epoch 26/100\n",
      "1/1 - 0s - 86ms/step - acc: 1.0000 - loss: 0.3463\n",
      "Epoch 27/100\n",
      "1/1 - 0s - 85ms/step - acc: 1.0000 - loss: 0.3386\n",
      "Epoch 28/100\n",
      "1/1 - 0s - 103ms/step - acc: 1.0000 - loss: 0.3311\n",
      "Epoch 29/100\n",
      "1/1 - 0s - 77ms/step - acc: 1.0000 - loss: 0.3239\n",
      "Epoch 30/100\n",
      "1/1 - 0s - 82ms/step - acc: 1.0000 - loss: 0.3169\n",
      "Epoch 31/100\n",
      "1/1 - 0s - 85ms/step - acc: 1.0000 - loss: 0.3101\n",
      "Epoch 32/100\n",
      "1/1 - 0s - 101ms/step - acc: 1.0000 - loss: 0.3035\n",
      "Epoch 33/100\n",
      "1/1 - 0s - 82ms/step - acc: 1.0000 - loss: 0.2971\n",
      "Epoch 34/100\n",
      "1/1 - 0s - 81ms/step - acc: 1.0000 - loss: 0.2909\n",
      "Epoch 35/100\n",
      "1/1 - 0s - 123ms/step - acc: 1.0000 - loss: 0.2849\n",
      "Epoch 36/100\n",
      "1/1 - 0s - 84ms/step - acc: 1.0000 - loss: 0.2790\n",
      "Epoch 37/100\n",
      "1/1 - 0s - 91ms/step - acc: 1.0000 - loss: 0.2734\n",
      "Epoch 38/100\n",
      "1/1 - 0s - 95ms/step - acc: 1.0000 - loss: 0.2679\n",
      "Epoch 39/100\n",
      "1/1 - 0s - 80ms/step - acc: 1.0000 - loss: 0.2625\n",
      "Epoch 40/100\n",
      "1/1 - 0s - 84ms/step - acc: 1.0000 - loss: 0.2573\n",
      "Epoch 41/100\n",
      "1/1 - 0s - 95ms/step - acc: 1.0000 - loss: 0.2523\n",
      "Epoch 42/100\n",
      "1/1 - 0s - 97ms/step - acc: 1.0000 - loss: 0.2474\n",
      "Epoch 43/100\n",
      "1/1 - 0s - 84ms/step - acc: 1.0000 - loss: 0.2426\n",
      "Epoch 44/100\n",
      "1/1 - 0s - 188ms/step - acc: 1.0000 - loss: 0.2380\n",
      "Epoch 45/100\n",
      "1/1 - 0s - 97ms/step - acc: 1.0000 - loss: 0.2335\n",
      "Epoch 46/100\n",
      "1/1 - 0s - 124ms/step - acc: 1.0000 - loss: 0.2292\n",
      "Epoch 47/100\n",
      "1/1 - 0s - 90ms/step - acc: 1.0000 - loss: 0.2249\n",
      "Epoch 48/100\n",
      "1/1 - 0s - 87ms/step - acc: 1.0000 - loss: 0.2208\n",
      "Epoch 49/100\n",
      "1/1 - 0s - 90ms/step - acc: 1.0000 - loss: 0.2168\n",
      "Epoch 50/100\n",
      "1/1 - 0s - 96ms/step - acc: 1.0000 - loss: 0.2129\n",
      "Epoch 51/100\n",
      "1/1 - 0s - 92ms/step - acc: 1.0000 - loss: 0.2091\n",
      "Epoch 52/100\n",
      "1/1 - 0s - 95ms/step - acc: 1.0000 - loss: 0.2054\n",
      "Epoch 53/100\n",
      "1/1 - 0s - 89ms/step - acc: 1.0000 - loss: 0.2018\n",
      "Epoch 54/100\n",
      "1/1 - 0s - 95ms/step - acc: 1.0000 - loss: 0.1983\n",
      "Epoch 55/100\n",
      "1/1 - 0s - 99ms/step - acc: 1.0000 - loss: 0.1949\n",
      "Epoch 56/100\n",
      "1/1 - 0s - 87ms/step - acc: 1.0000 - loss: 0.1916\n",
      "Epoch 57/100\n",
      "1/1 - 0s - 80ms/step - acc: 1.0000 - loss: 0.1884\n",
      "Epoch 58/100\n",
      "1/1 - 0s - 91ms/step - acc: 1.0000 - loss: 0.1853\n",
      "Epoch 59/100\n",
      "1/1 - 0s - 93ms/step - acc: 1.0000 - loss: 0.1822\n",
      "Epoch 60/100\n",
      "1/1 - 0s - 101ms/step - acc: 1.0000 - loss: 0.1792\n",
      "Epoch 61/100\n",
      "1/1 - 0s - 86ms/step - acc: 1.0000 - loss: 0.1763\n",
      "Epoch 62/100\n",
      "1/1 - 0s - 112ms/step - acc: 1.0000 - loss: 0.1734\n",
      "Epoch 63/100\n",
      "1/1 - 0s - 174ms/step - acc: 1.0000 - loss: 0.1707\n",
      "Epoch 64/100\n",
      "1/1 - 0s - 154ms/step - acc: 1.0000 - loss: 0.1680\n",
      "Epoch 65/100\n",
      "1/1 - 0s - 181ms/step - acc: 1.0000 - loss: 0.1653\n",
      "Epoch 66/100\n",
      "1/1 - 0s - 125ms/step - acc: 1.0000 - loss: 0.1628\n",
      "Epoch 67/100\n",
      "1/1 - 0s - 114ms/step - acc: 1.0000 - loss: 0.1603\n",
      "Epoch 68/100\n",
      "1/1 - 0s - 111ms/step - acc: 1.0000 - loss: 0.1578\n",
      "Epoch 69/100\n",
      "1/1 - 0s - 124ms/step - acc: 1.0000 - loss: 0.1554\n",
      "Epoch 70/100\n",
      "1/1 - 0s - 100ms/step - acc: 1.0000 - loss: 0.1531\n",
      "Epoch 71/100\n",
      "1/1 - 0s - 101ms/step - acc: 1.0000 - loss: 0.1508\n",
      "Epoch 72/100\n",
      "1/1 - 0s - 109ms/step - acc: 1.0000 - loss: 0.1486\n",
      "Epoch 73/100\n",
      "1/1 - 0s - 101ms/step - acc: 1.0000 - loss: 0.1464\n",
      "Epoch 74/100\n",
      "1/1 - 0s - 100ms/step - acc: 1.0000 - loss: 0.1443\n",
      "Epoch 75/100\n",
      "1/1 - 0s - 93ms/step - acc: 1.0000 - loss: 0.1422\n",
      "Epoch 76/100\n",
      "1/1 - 0s - 107ms/step - acc: 1.0000 - loss: 0.1402\n",
      "Epoch 77/100\n",
      "1/1 - 0s - 127ms/step - acc: 1.0000 - loss: 0.1382\n",
      "Epoch 78/100\n",
      "1/1 - 0s - 102ms/step - acc: 1.0000 - loss: 0.1363\n",
      "Epoch 79/100\n",
      "1/1 - 0s - 105ms/step - acc: 1.0000 - loss: 0.1344\n",
      "Epoch 80/100\n",
      "1/1 - 0s - 91ms/step - acc: 1.0000 - loss: 0.1325\n",
      "Epoch 81/100\n",
      "1/1 - 0s - 95ms/step - acc: 1.0000 - loss: 0.1307\n",
      "Epoch 82/100\n",
      "1/1 - 0s - 98ms/step - acc: 1.0000 - loss: 0.1289\n",
      "Epoch 83/100\n",
      "1/1 - 0s - 106ms/step - acc: 1.0000 - loss: 0.1272\n",
      "Epoch 84/100\n",
      "1/1 - 0s - 100ms/step - acc: 1.0000 - loss: 0.1255\n",
      "Epoch 85/100\n",
      "1/1 - 0s - 98ms/step - acc: 1.0000 - loss: 0.1238\n",
      "Epoch 86/100\n",
      "1/1 - 0s - 92ms/step - acc: 1.0000 - loss: 0.1222\n",
      "Epoch 87/100\n",
      "1/1 - 0s - 89ms/step - acc: 1.0000 - loss: 0.1206\n",
      "Epoch 88/100\n",
      "1/1 - 0s - 100ms/step - acc: 1.0000 - loss: 0.1190\n",
      "Epoch 89/100\n",
      "1/1 - 0s - 98ms/step - acc: 1.0000 - loss: 0.1175\n",
      "Epoch 90/100\n",
      "1/1 - 0s - 101ms/step - acc: 1.0000 - loss: 0.1160\n",
      "Epoch 91/100\n",
      "1/1 - 0s - 102ms/step - acc: 1.0000 - loss: 0.1145\n",
      "Epoch 92/100\n",
      "1/1 - 0s - 105ms/step - acc: 1.0000 - loss: 0.1131\n",
      "Epoch 93/100\n",
      "1/1 - 0s - 96ms/step - acc: 1.0000 - loss: 0.1116\n",
      "Epoch 94/100\n",
      "1/1 - 0s - 95ms/step - acc: 1.0000 - loss: 0.1103\n",
      "Epoch 95/100\n",
      "1/1 - 0s - 108ms/step - acc: 1.0000 - loss: 0.1089\n",
      "Epoch 96/100\n",
      "1/1 - 0s - 107ms/step - acc: 1.0000 - loss: 0.1076\n",
      "Epoch 97/100\n",
      "1/1 - 0s - 103ms/step - acc: 1.0000 - loss: 0.1063\n",
      "Epoch 98/100\n",
      "1/1 - 0s - 99ms/step - acc: 1.0000 - loss: 0.1050\n",
      "Epoch 99/100\n",
      "1/1 - 0s - 125ms/step - acc: 1.0000 - loss: 0.1037\n",
      "Epoch 100/100\n",
      "1/1 - 0s - 94ms/step - acc: 1.0000 - loss: 0.1025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x159a5c81390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape = (max_len, ), dtype = 'int32'))\n",
    "e = Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_len, trainable = False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.fit(X_train, y_train, epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a39b8-380d-402b-b240-85dc0b763487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
